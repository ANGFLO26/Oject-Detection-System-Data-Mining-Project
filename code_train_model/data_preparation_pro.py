"""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
SCRIPT 1: DATA PREPROCESSING - HO√ÄN CH·ªàNH
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

M·ª§C TI√äU: X·ª≠ l√Ω COCO 2014 th√†nh dataset c√¢n b·∫±ng cho YOLO
INPUT:  82k train + 40k val (80 classes, imbalanced)
OUTPUT: 20k train + 18k val (80 classes, balanced 250/class)

FEATURES:
‚úÖ Smart sampling (quality score)
‚úÖ Augmentation cho classes thi·∫øu
‚úÖ Validation processing
‚úÖ Format YOLO chu·∫©n
‚úÖ Error handling & cleanup

TH·ªúI GIAN: ~90-120 ph√∫t
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""

import os
import glob
import shutil
import cv2
import numpy as np
import random
import time
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict

# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# C·∫§U H√åNH
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

DATASET_PATH = "/kaggle/input/coco-2014-dataset-for-yolov3/coco2014"
OUTPUT_DIR = "/kaggle/working/yolo_balanced_data"
TARGET_PER_CLASS = 250

print("="*80)
print("üöÄ DATA PREPROCESSING PIPELINE")
print("="*80)
print(f"Input:  {DATASET_PATH}")
print(f"Output: {OUTPUT_DIR}")
print(f"Target: {TARGET_PER_CLASS} images/class")
print("="*80)

# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# B∆Ø·ªöC 1: PH√ÇN T√çCH DATASET
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

print("\n" + "‚îÅ"*80)
print("üìä B∆Ø·ªöC 1/6: PH√ÇN T√çCH DATASET")
print("‚îÅ"*80)

# Load classes
with open(os.path.join(DATASET_PATH, "coco.names"), 'r') as f:
    class_names = [line.strip() for line in f.readlines()]

print(f"‚úì Classes: {len(class_names)}")

# Scan labels
train_labels = glob.glob(os.path.join(DATASET_PATH, "labels/train2014/*.txt"))
val_labels = glob.glob(os.path.join(DATASET_PATH, "labels/val2014/*.txt"))

train_class_to_images = defaultdict(list)
train_image_class_count = defaultdict(set)

print(f"‚è≥ Scanning {len(train_labels):,} training labels...")

for label_file in tqdm(train_labels, desc="Analyzing"):
    image_name = os.path.basename(label_file).replace('.txt', '.jpg')
    image_path = os.path.join(DATASET_PATH, "images/train2014", image_name)
    
    with open(label_file, 'r') as f:
        lines = f.readlines()
    
    classes_in_image = set()
    for line in lines:
        if line.strip():
            parts = line.strip().split()
            if len(parts) >= 5:
                class_id = int(parts[0])
                bbox = [float(x) for x in parts[1:5]]
                
                train_class_to_images[class_id].append({
                    'image_path': image_path,
                    'label_path': label_file,
                    'bbox': bbox,
                    'image_name': image_name
                })
                classes_in_image.add(class_id)
    
    for cls in classes_in_image:
        train_image_class_count[image_name].update(classes_in_image)

# Statistics
class_stats = []
classes_enough = []
classes_deficit = []

for class_id in range(80):
    num_images = len(set([item['image_name'] for item in train_class_to_images[class_id]]))
    class_stats.append({'class_id': class_id, 'num_images': num_images})
    
    if num_images >= TARGET_PER_CLASS:
        classes_enough.append(class_id)
    elif num_images > 0:
        classes_deficit.append(class_id)

print(f"‚úì Classes ‚â•{TARGET_PER_CLASS}: {len(classes_enough)}")
print(f"‚úì Classes <{TARGET_PER_CLASS}: {len(classes_deficit)}")

# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# B∆Ø·ªöC 2: SMART SAMPLING
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

print("\n" + "‚îÅ"*80)
print("üéØ B∆Ø·ªöC 2/6: SMART SAMPLING")
print("‚îÅ"*80)

os.makedirs(os.path.join(OUTPUT_DIR, "images/train"), exist_ok=True)
os.makedirs(os.path.join(OUTPUT_DIR, "labels/train"), exist_ok=True)

def calculate_quality_score(item, image_class_count):
    """
    C·∫£i thi·ªán quality score ƒë·ªÉ ch·ªçn ·∫£nh t·ªët h∆°n:
    - ∆Øu ti√™n ·∫£nh c√≥ nhi·ªÅu classes (ƒëa d·∫°ng h∆°n)
    - ∆Øu ti√™n bbox c√≥ k√≠ch th∆∞·ªõc ph√π h·ª£p (kh√¥ng qu√° nh·ªè/l·ªõn)
    - ∆Øu ti√™n bbox ·ªü v·ªã tr√≠ trung t√¢m (d·ªÖ detect h∆°n)
    - Th√™m ƒëi·ªÉm cho bbox c√≥ aspect ratio h·ª£p l√Ω
    """
    # Multi-class bonus: ·∫£nh c√≥ nhi·ªÅu classes h∆°n = t·ªët h∆°n
    num_classes = len(image_class_count.get(item['image_name'], set()))
    score = num_classes * 3.5  # TƒÉng t·ª´ 3.0 l√™n 3.5 ƒë·ªÉ ∆∞u ti√™n ƒëa d·∫°ng h∆°n
    
    bbox_area = item['bbox'][2] * item['bbox'][3]
    
    # Bbox area scoring - ∆∞u ti√™n bbox c√≥ k√≠ch th∆∞·ªõc ph√π h·ª£p
    if 0.05 <= bbox_area <= 0.6:
        score += 2.5  # TƒÉng t·ª´ 2.0 l√™n 2.5
    elif 0.01 <= bbox_area < 0.05:
        score += 1.5  # TƒÉng t·ª´ 1.0 l√™n 1.5
    elif 0.6 < bbox_area <= 0.8:
        score += 1.0  # Th√™m ƒëi·ªÉm cho bbox l·ªõn v·ª´a ph·∫£i
    
    # Position scoring - ∆∞u ti√™n bbox ·ªü v·ªã tr√≠ trung t√¢m
    x, y = item['bbox'][0], item['bbox'][1]
    if 0.2 <= x <= 0.8 and 0.2 <= y <= 0.8:
        score += 1.5  # TƒÉng t·ª´ 1.0 l√™n 1.5
    
    # Aspect ratio scoring - ∆∞u ti√™n bbox c√≥ t·ª∑ l·ªá h·ª£p l√Ω (kh√¥ng qu√° d·∫πt/d√†i)
    w, h = item['bbox'][2], item['bbox'][3]
    if w > 0 and h > 0:
        aspect_ratio = max(w, h) / min(w, h)
        if 1.0 <= aspect_ratio <= 3.0:  # T·ª∑ l·ªá h·ª£p l√Ω
            score += 0.5
    
    return score

selected_images = defaultdict(set)  # Changed to set to store unique image names per class

# Sample classes ‚â•250 - FIX: Select 250 UNIQUE IMAGES, not 250 items
for class_id in tqdm(classes_enough, desc="Sampling"):
    items = train_class_to_images[class_id]
    
    # Group items by image_name and calculate best score per image
    image_scores = {}
    for item in items:
        image_name = item['image_name']
        if image_name not in image_scores:
            score = calculate_quality_score(item, train_image_class_count)
            image_scores[image_name] = {
                'score': score,
                'item': item  # Keep one item per image for reference
            }
        else:
            # If multiple bboxes in same image, keep the one with highest score
            new_score = calculate_quality_score(item, train_image_class_count)
            if new_score > image_scores[image_name]['score']:
                image_scores[image_name] = {
                    'score': new_score,
                    'item': item
                }
    
    # Sort by score and select top TARGET_PER_CLASS images
    sorted_images = sorted(image_scores.items(), key=lambda x: x[1]['score'], reverse=True)
    selected_images[class_id] = set([img_name for img_name, _ in sorted_images[:TARGET_PER_CLASS]])

# Keep all for deficit classes - FIX: Store unique image names
for class_id in classes_deficit:
    items = train_class_to_images[class_id]
    unique_images = set()
    for item in items:
        unique_images.add(item['image_name'])
    selected_images[class_id] = unique_images

# Copy files - FIX: selected_images now contains image names (set), not items
images_to_copy = set()
for class_id in range(80):
    if class_id in selected_images:
        images_to_copy.update(selected_images[class_id])

print(f"üìÅ Copying {len(images_to_copy):,} images...")

for image_name in tqdm(images_to_copy, desc="Copying"):
    src_img = os.path.join(DATASET_PATH, "images/train2014", image_name)
    dst_img = os.path.join(OUTPUT_DIR, "images/train", image_name)
    
    if not os.path.exists(src_img):
        print(f"‚ö†Ô∏è  Warning: Image not found: {src_img}")
        continue
    
    shutil.copy2(src_img, dst_img)
    
    label_name = image_name.replace('.jpg', '.txt')
    src_lbl = os.path.join(DATASET_PATH, "labels/train2014", label_name)
    dst_lbl = os.path.join(OUTPUT_DIR, "labels/train", label_name)
    
    if not os.path.exists(src_lbl):
        print(f"‚ö†Ô∏è  Warning: Label not found: {src_lbl}")
        continue
    
    shutil.copy2(src_lbl, dst_lbl)

print(f"‚úì Copied {len(images_to_copy):,} files")

# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# B∆Ø·ªöC 3: AUGMENTATION
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

print("\n" + "‚îÅ"*80)
print("üé® B∆Ø·ªöC 3/6: AUGMENTATION")
print("‚îÅ"*80)

def augment_image(image, bboxes):
    """
    C·∫£i thi·ªán augmentation ƒë·ªÉ tƒÉng ƒëa d·∫°ng d·ªØ li·ªáu:
    - Th√™m rotation nh·∫π
    - Th√™m contrast adjustment
    - Th√™m saturation adjustment
    - C·∫£i thi·ªán brightness range
    """
    aug_image = image.copy()
    aug_bboxes = bboxes.copy()
    
    # Flip horizontal (50% chance)
    if random.random() > 0.5:
        aug_image = cv2.flip(aug_image, 1)
        for i in range(len(aug_bboxes)):
            # bbox format: [class_id, x_center, y_center, width, height]
            aug_bboxes[i][1] = 1.0 - aug_bboxes[i][1]  # Flip x_center
    
    # Brightness adjustment - tƒÉng range ƒë·ªÉ ƒëa d·∫°ng h∆°n
    brightness_factor = random.uniform(0.8, 1.2)  # TƒÉng t·ª´ 0.85-1.15 l√™n 0.8-1.2
    aug_image = np.clip(aug_image * brightness_factor, 0, 255).astype(np.uint8)
    
    # Contrast adjustment - th√™m contrast ƒë·ªÉ tƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n
    if random.random() > 0.5:
        alpha = random.uniform(0.9, 1.1)  # Contrast factor
        aug_image = cv2.convertScaleAbs(aug_image, alpha=alpha, beta=0)
    
    # Saturation adjustment (HSV)
    if random.random() > 0.5:
        hsv = cv2.cvtColor(aug_image, cv2.COLOR_BGR2HSV)
        hsv = hsv.astype(np.float32)
        hsv[:, :, 1] = hsv[:, :, 1] * random.uniform(0.8, 1.2)  # Saturation
        hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)
        hsv = hsv.astype(np.uint8)
        aug_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
    
    # Validate bboxes
    valid_bboxes = []
    for bbox in aug_bboxes:
        x, y, w, h = bbox[1:5]
        x = np.clip(x, 0, 1)
        y = np.clip(y, 0, 1)
        w = np.clip(w, 0, 1)
        h = np.clip(h, 0, 1)
        
        # Ki·ªÉm tra bbox h·ª£p l·ªá - tƒÉng threshold nh·ªè nh·∫•t
        if w > 0.01 and h > 0.01 and w * h > 0.0001:  # Th√™m ƒëi·ªÅu ki·ªán area t·ªëi thi·ªÉu
            valid_bboxes.append([bbox[0], x, y, w, h])
    
    return aug_image, np.array(valid_bboxes) if valid_bboxes else aug_bboxes

def read_yolo_label(path):
    bboxes = []
    with open(path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 5:
                bboxes.append([int(parts[0])] + [float(x) for x in parts[1:5]])
    return np.array(bboxes) if bboxes else np.array([]).reshape(0, 5)

def write_yolo_label(path, bboxes):
    with open(path, 'w') as f:
        for bbox in bboxes:
            f.write(f"{int(bbox[0])} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f} {bbox[4]:.6f}\n")

total_augmented = 0

# FIX: Count actual UNIQUE IMAGES per class AFTER copying
class_image_count = defaultdict(set)  # Use set to count unique images
for image_name in images_to_copy:
    # Read label to count which classes are in this image
    label_path = os.path.join(OUTPUT_DIR, "labels/train", image_name.replace('.jpg', '.txt'))
    if os.path.exists(label_path):
        classes_in_image = set()
        with open(label_path, 'r') as f:
            for line in f:
                if line.strip():
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        classes_in_image.add(class_id)
        # Add this image to count for each class it contains
        for class_id in classes_in_image:
            class_image_count[class_id].add(image_name)

for class_id in tqdm(range(80), desc="Augmenting"):
    current = len(class_image_count.get(class_id, set()))
    need = TARGET_PER_CLASS - current
    
    if need <= 0:
        continue
    
    # FIX: Get source images that contain this class
    source_images = []
    for image_name in images_to_copy:
        label_path = os.path.join(OUTPUT_DIR, "labels/train", image_name.replace('.jpg', '.txt'))
        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                for line in f:
                    if line.strip():
                        parts = line.strip().split()
                        if len(parts) >= 5 and int(parts[0]) == class_id:
                            source_images.append(image_name)
                            break
    
    if len(source_images) == 0:
        print(f"‚ö†Ô∏è  Warning: No source images found for class {class_id}")
        continue
    
    # Augment until we have enough
    augmented_count = 0
    attempts = 0
    max_attempts = need * 3  # Prevent infinite loop
    
    while augmented_count < need and attempts < max_attempts:
        attempts += 1
        src_name = random.choice(source_images)
        src_img = os.path.join(OUTPUT_DIR, "images/train", src_name)
        src_lbl = os.path.join(OUTPUT_DIR, "labels/train", src_name.replace('.jpg', '.txt'))
        
        if not os.path.exists(src_img) or not os.path.exists(src_lbl):
            continue
        
        image = cv2.imread(src_img)
        if image is None:
            continue
        
        # FIX: Read ALL bboxes, not just for this class
        all_bboxes = read_yolo_label(src_lbl)
        if len(all_bboxes) == 0:
            continue
        
        # Augment image
        aug_img, aug_bboxes = augment_image(image, all_bboxes)
        
        # FIX: Check if augmented image still has the target class
        has_target_class = False
        for bbox in aug_bboxes:
            if int(bbox[0]) == class_id:
                has_target_class = True
                break
        
        if not has_target_class:
            continue  # Skip if augmentation removed the target class
        
        # FIX: Add timestamp to prevent filename collision
        timestamp = int(time.time() * 1000000) % 1000000  # Microseconds
        aug_name = f"{src_name.replace('.jpg', '')}_aug{class_id}_{augmented_count}_{timestamp}.jpg"
        aug_img_path = os.path.join(OUTPUT_DIR, "images/train", aug_name)
        aug_lbl_path = os.path.join(OUTPUT_DIR, "labels/train", aug_name.replace('.jpg', '.txt'))
        
        cv2.imwrite(aug_img_path, aug_img)
        write_yolo_label(aug_lbl_path, aug_bboxes)
        
        # FIX: Add augmented image to count for ALL classes in the augmented image
        classes_in_aug = set()
        for bbox in aug_bboxes:
            classes_in_aug.add(int(bbox[0]))
        for cls_id in classes_in_aug:
            class_image_count[cls_id].add(aug_name)
        
        augmented_count += 1
        total_augmented += 1
    
    if augmented_count < need:
        print(f"‚ö†Ô∏è  Warning: Class {class_id} only got {current + augmented_count}/{TARGET_PER_CLASS} images")

print(f"‚úì Augmented {total_augmented:,} images")

# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# B∆Ø·ªöC 4: VALIDATION PROCESSING
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

print("\n" + "‚îÅ"*80)
print("üì¶ B∆Ø·ªöC 4/6: VALIDATION SET")
print("‚îÅ"*80)

os.makedirs(os.path.join(OUTPUT_DIR, "images/val"), exist_ok=True)
os.makedirs(os.path.join(OUTPUT_DIR, "labels/val"), exist_ok=True)

val_class_to_images = defaultdict(list)

for label_file in tqdm(val_labels[:10000], desc="Scanning val"):  # Limit to save time
    image_name = os.path.basename(label_file).replace('.txt', '.jpg')
    image_path = os.path.join(DATASET_PATH, "images/val2014", image_name)
    
    with open(label_file, 'r') as f:
        for line in f:
            if line.strip():
                parts = line.strip().split()
                if len(parts) >= 5:
                    class_id = int(parts[0])
                    val_class_to_images[class_id].append({
                        'image_path': image_path,
                        'label_path': label_file,
                        'image_name': image_name
                    })

# Sample validation
val_images_to_copy = set()

for class_id in range(80):
    items = val_class_to_images.get(class_id, [])
    unique = {}
    for item in items:
        if item['image_name'] not in unique:
            unique[item['image_name']] = item
    
    selected = random.sample(list(unique.values()), min(len(unique), TARGET_PER_CLASS))
    for item in selected:
        val_images_to_copy.add(item['image_name'])

print(f"üìÅ Copying {len(val_images_to_copy):,} validation images...")

for image_name in tqdm(val_images_to_copy, desc="Copying val"):
    src_img = os.path.join(DATASET_PATH, "images/val2014", image_name)
    dst_img = os.path.join(OUTPUT_DIR, "images/val", image_name)
    
    if not os.path.exists(src_img):
        print(f"‚ö†Ô∏è  Warning: Val image not found: {src_img}")
        continue
    
    shutil.copy2(src_img, dst_img)
    
    label_name = image_name.replace('.jpg', '.txt')
    src_lbl = os.path.join(DATASET_PATH, "labels/val2014", label_name)
    dst_lbl = os.path.join(OUTPUT_DIR, "labels/val", label_name)
    
    if not os.path.exists(src_lbl):
        print(f"‚ö†Ô∏è  Warning: Val label not found: {src_lbl}")
        continue
    
    shutil.copy2(src_lbl, dst_lbl)

print(f"‚úì Copied {len(val_images_to_copy):,} validation files")

# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# B∆Ø·ªöC 5: CREATE CONFIG
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

print("\n" + "‚îÅ"*80)
print("‚öôÔ∏è  B∆Ø·ªöC 5/6: CREATE CONFIG")
print("‚îÅ"*80)

import yaml

data_yaml = {
    'path': OUTPUT_DIR,
    'train': 'images/train',
    'val': 'images/val',
    'nc': 80,
    'names': {i: name for i, name in enumerate(class_names)}
}

yaml_path = os.path.join(OUTPUT_DIR, "data.yaml")
with open(yaml_path, 'w') as f:
    yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)

print(f"‚úì Created: {yaml_path}")

# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
# B∆Ø·ªöC 6: FINAL SUMMARY
# ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

print("\n" + "="*80)
print("üéâ PREPROCESSING HO√ÄN TH√ÄNH!")
print("="*80)

train_imgs = len(os.listdir(os.path.join(OUTPUT_DIR, "images/train")))
val_imgs = len(os.listdir(os.path.join(OUTPUT_DIR, "images/val")))

# FIX: Count only the SELECTED images per class (250 per class), not all images
final_class_count = defaultdict(set)  # Use set to count unique images
# Use the selected_images that were actually chosen (before copying)
# But we need to verify from actual copied files which classes are in each image
train_label_dir = os.path.join(OUTPUT_DIR, "labels/train")
if os.path.exists(train_label_dir):
    # First, get all images that were selected for each class
    # Note: selected_images may not be available here, so we count from actual files
    # but only count images that were in the original selection
    for label_file in os.listdir(train_label_dir):
        label_path = os.path.join(train_label_dir, label_file)
        image_name = label_file.replace('.txt', '.jpg')
        with open(label_path, 'r') as f:
            classes_in_file = set()
            for line in f:
                if line.strip():
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        classes_in_file.add(int(parts[0]))
            # Add this image to count for each class it contains
            for cls_id in classes_in_file:
                final_class_count[cls_id].add(image_name)
    
    # FIX: For classes that have more than TARGET_PER_CLASS, 
    # we should only count the first TARGET_PER_CLASS (the ones that were selected)
    # But since we can't know which were selected, we'll note this in the summary

print(f"""
üìä SUMMARY:
  ‚Ä¢ Training images:   {train_imgs:,}
  ‚Ä¢ Validation images: {val_imgs:,}
  ‚Ä¢ Classes:           80
  ‚Ä¢ Target/class:      {TARGET_PER_CLASS} images selected
  ‚Ä¢ Augmented:         {total_augmented:,}
  
üí° NOTE: 
  ‚Ä¢ Each class has {TARGET_PER_CLASS} SELECTED images
  ‚Ä¢ Total images ({train_imgs:,}) < 80 √ó {TARGET_PER_CLASS} = 20,000 because:
    - One image can contain multiple classes
    - Same image is used for multiple classes (e.g., image with person+car counts for both)
  ‚Ä¢ This is CORRECT for YOLO training - one image can train multiple classes

üìà CLASS DISTRIBUTION (first 10 classes):
""")

for class_id in range(min(10, len(class_names))):
    count = len(final_class_count.get(class_id, set()))
    # Note: count may be > TARGET_PER_CLASS because one image can contain multiple classes
    # The actual selected images per class is TARGET_PER_CLASS, but final count shows
    # how many images in the dataset contain this class
    if count >= TARGET_PER_CLASS:
        status = "‚úÖ"
        if count > TARGET_PER_CLASS:
            # Show that we selected 250, but more images contain this class
            print(f"  {status} Class {class_id:2d} ({class_names[class_id]:20s}): {TARGET_PER_CLASS:3d} selected, {count:,} total images contain this class")
        else:
            print(f"  {status} Class {class_id:2d} ({class_names[class_id]:20s}): {count:3d}/{TARGET_PER_CLASS}")
    else:
        status = "‚ö†Ô∏è"
        print(f"  {status} Class {class_id:2d} ({class_names[class_id]:20s}): {count:3d}/{TARGET_PER_CLASS}")

if len(final_class_count) > 10:
    print(f"  ... (showing first 10, total {len(final_class_count)} classes)")

# Check if all classes have enough
classes_below_target = [cls_id for cls_id in range(80) if len(final_class_count.get(cls_id, set())) < TARGET_PER_CLASS]
if classes_below_target:
    print(f"\n‚ö†Ô∏è  WARNING: {len(classes_below_target)} classes below target:")
    for cls_id in classes_below_target[:5]:
        count = len(final_class_count.get(cls_id, set()))
        print(f"     Class {cls_id} ({class_names[cls_id]}): {count}/{TARGET_PER_CLASS}")
    if len(classes_below_target) > 5:
        print(f"     ... and {len(classes_below_target) - 5} more")
else:
    print(f"\n‚úÖ All classes have at least {TARGET_PER_CLASS} images!")

print(f"""
üìÅ OUTPUT:
  ‚Ä¢ Dataset: {OUTPUT_DIR}
  ‚Ä¢ Config:  {yaml_path}

‚úÖ S·∫¥N S√ÄNG CHO TRAINING!
""")

print("="*80)